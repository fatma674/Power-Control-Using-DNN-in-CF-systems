{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d748f2-7a5b-41e8-b81d-55c32e84ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, BatchNormalization, ReLU, Add,\n",
    "    Flatten, Reshape, Activation\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_dataset(M, K, T):\n",
    "    Path_Beta_mk = \"D:\\OneDrive - Assuit University\\My_files\\Master Material\\Papers\\P5_Joint_Powercontrol_PilotAssignement\\Beta_mk_46712.csv\"\n",
    "    Path_Eitta_mk = \"D:\\OneDrive - Assuit University\\My_files\\Master Material\\Papers\\P5_Joint_Powercontrol_PilotAssignement\\Eitta_mk_46712.csv\"\n",
    "    \n",
    "    #  Load data with proper headers\n",
    "    Beta_mk_T = pd.read_csv(Path_Beta_mk, header=None)\n",
    "    Eitta_mk_T = pd.read_csv(Path_Eitta_mk, header=None)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    Beta_mk = Beta_mk_T.values.reshape(-1, M, K)\n",
    "    Eitta_mk = Eitta_mk_T.values.reshape(-1, M, K)\n",
    "    \n",
    "    # Verify shapes match expected dimensions\n",
    "    assert Beta_mk.shape == (T, M, K), f\"Beta shape mismatch. Got {Beta_mk.shape}, expected ({T}, {M}, {K})\"\n",
    "    print('beta',Beta_mk[0])\n",
    "    print('eitta',Eitta_mk[0] )\n",
    "    Beta_mk = Beta_mk[:46700, :, :]\n",
    "    Eitta_mk = Eitta_mk[:46700, :, :]\n",
    "    return Beta_mk, Eitta_mk\n",
    "    print('done 1') \n",
    "\n",
    "try:\n",
    "    Beta_mk, Eitta_mk = load_dataset(25, 6, 46712)\n",
    "    print(\"Data loaded successfully!\")\n",
    "    print(f\"Beta shape: {Beta_mk.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49e2f3-d130-4ec0-ab7b-10429605dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D,Conv1D, BatchNormalization, ReLU, Add,\n",
    "    GlobalAveragePooling2D, Dense, Reshape, Activation,Dropout\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Multiply, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Multiply, Concatenate, Conv2D\n",
    "\n",
    "\n",
    "def se_block(x, ratio=16):\n",
    "    channels = x.shape[-1]\n",
    "    se = GlobalAveragePooling2D()(x)\n",
    "    se = Dense(channels // ratio, activation='relu')(se)\n",
    "    se = Dense(channels, activation='sigmoid')(se)\n",
    "    return Multiply()([x, se])\n",
    "\n",
    "def cbam_block(x, ratio=16):\n",
    "    # Channel Attention\n",
    "    channel = GlobalAveragePooling2D()(x)\n",
    "    channel = Dense(x.shape[-1] // ratio, activation='relu')(channel)\n",
    "    channel = Dense(x.shape[-1], activation='sigmoid')(channel)\n",
    "    x = Multiply()([x, channel])\n",
    "    \n",
    "    # Spatial Attention\n",
    "    spatial = Conv2D(1, (7,7), padding='same', activation='sigmoid')(x)\n",
    "    x = Multiply()([x, spatial])\n",
    "    return x\n",
    "\n",
    "def resnet_block(x, filters, kernel_size=(7, 7), name=None):\n",
    "    \"\"\"Residual block with proper Sum connection\"\"\"\n",
    "    shortcut = x\n",
    "    \n",
    "    # Main path\n",
    "    x = Conv2D(filters, kernel_size, padding='same', name=f'{name}_conv1')(x)\n",
    "    x = BatchNormalization(name=f'{name}_bn1')(x)\n",
    "    x = ReLU(name=f'{name}_relu1')(x)\n",
    "    \n",
    "    x = Conv2D(filters, kernel_size, padding='same', name=f'{name}_conv2')(x)\n",
    "    x = BatchNormalization(name=f'{name}_bn2')(x)\n",
    "    \n",
    "    # Shortcut connection\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = Conv2D(filters, (1, 1), name=f'{name}_shortcut')(shortcut)\n",
    "        shortcut = BatchNormalization(name=f'{name}_shortcut_bn')(shortcut)\n",
    "    \n",
    "    x = Add(name=f'{name}_sum')([x, shortcut])\n",
    "    x = ReLU(name=f'{name}_out')(x)\n",
    "    return x\n",
    "\n",
    "def build_joint_PC_model(input_shape, M, K, P,RESNET_BLOCKS , NO_Filters):  \n",
    "    inputs = Input(shape=input_shape, name='LSF_Input')  # (25, 1, 6)\n",
    "    # Conv Block 1\n",
    "    x = Conv2D(NO_Filters, (7, 7), padding='same', name='conv_block1')(inputs)\n",
    "    x = BatchNormalization(momentum=0.99, name='bn1')(x)\n",
    "    x = ReLU(name='relu1')(x)\n",
    "    \n",
    "    # ResNet Blocks (R=3)\n",
    "    for i in range(RESNET_BLOCKS):\n",
    "        x = resnet_block(x, NO_Filters, name=f'resnet{i+1}')\n",
    "\n",
    "    #x = se_block(x, ratio=16)  # Add SE block\n",
    "    x = cbam_block(x, ratio=16)\n",
    "    # Conv Block 2\n",
    "    x = Conv2D(NO_Filters, (7, 7), padding='same', name='conv_block2',kernel_regularizer=l2(1e-4))(x)\n",
    "    x = BatchNormalization(momentum=0.99, name='bn2')(x)\n",
    "    x = ReLU(name='relu2')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "\n",
    "    # ======== Power Control Branch ========\n",
    "    pc = Conv2D(K, (7, 7), padding='same', name='power_conv',kernel_regularizer=l2(1e-4))(x)\n",
    "    pc = Reshape((M, K), name='power_output_1')(pc)\n",
    "    pc = Activation('linear',name='power_output' )(pc)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=[ pc])\n",
    "# Parameters\n",
    "M, K, P = 25, 6, 2  # APs, Users, Pilots\n",
    "RESNET_BLOCKS =4\n",
    "NO_Filters = 512\n",
    "input_shape = (M, 1, K)  # Height, Width, Channels\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 256\n",
    "# ==============================================\n",
    "# Model Compilation\n",
    "# ==============================================\n",
    "PC_model_5 = build_joint_PC_model(input_shape, M, K, P ,RESNET_BLOCKS,NO_Filters)\n",
    "\n",
    "def hybrid_loss(y_true, y_pred, delta=0.1, alpha=0.5):\n",
    "    huber = tf.keras.losses.Huber(delta=delta)(y_true, y_pred)\n",
    "    msle = tf.keras.losses.MeanSquaredLogarithmicError()(y_true, y_pred)\n",
    "    return alpha * huber + (1 - alpha) * msle\n",
    "\n",
    "# Update model compilation\n",
    "PC_model_5.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4, clipnorm=1.0),\n",
    "    loss={'power_output': hybrid_loss},\n",
    "    metrics={'power_output': ['mae']}\n",
    ")\n",
    "\n",
    "print(\"\\nModel Output Names:\", PC_model_5.output_names)\n",
    "print(\"Model Output Shapes:\", [output.shape for output in PC_model_5.outputs])\n",
    "\n",
    "print(PC_model_5.output_names)\n",
    "print([output.shape for output in PC_model_5.outputs])\n",
    "PC_model_5.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c4e168-772b-4c08-a962-897b932408bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, BatchNormalization, ReLU, Add,\n",
    "    GlobalAveragePooling2D, Dense, Reshape, Activation)\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import PowerTransformer  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Train/validation split\n",
    "# Split both X and y together\n",
    "X_train, X_val, ypc_train, ypc_val = train_test_split(\n",
    "    Beta_mk, \n",
    "    Eitta_mk, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "#split_idx = int(0.8 * 46700)\n",
    "#X_train, X_val = Beta_mk[:split_idx], Beta_mk[split_idx:]\n",
    "#ypc_train, ypc_val = Eitta_mk[:split_idx], Eitta_mk[split_idx:]   \n",
    "\n",
    "print('xshape' , X_train.shape)\n",
    "print('yshape',ypc_train.shape)\n",
    "# Inputs (Beta_mk)\n",
    "#scaler_X_5 = MinMaxScaler()\n",
    "scaler_X_5 = QuantileTransformer(n_quantiles=1000, output_distribution='normal')\n",
    "#X_train= np.where(X_train <= 0, 1e-30, X_train)\n",
    "#X_train_log = np.log(X_train)\n",
    "X_train_normalized = scaler_X_5.fit_transform(X_train.reshape(-1, K))\n",
    "X_train_normalized = X_train_normalized.reshape(-1, M, 1, K)  # (samples, M, 1, K)\n",
    "\n",
    "#X_val= np.where(X_val <= 0, 1e-30, X_val)\n",
    "#X_val_log = np.log(X_val)\n",
    "X_val_normalized = scaler_X_5.transform(X_val.reshape(-1, K))\n",
    "X_val_normalized = X_val_normalized.reshape(-1, M, 1, K)\n",
    "\n",
    "# Targets (Eitta_mk)\n",
    "#scaler_y_5 = MinMaxScaler()\n",
    "scaler_y_5 = PowerTransformer(method='yeo-johnson')\n",
    "#ypc_train_scaled = scaler_y_5.fit_transform(ypc_train.reshape(-1, K))\n",
    "# For training targets\n",
    "ypc_train_scaled = scaler_y_5.fit_transform(ypc_train.reshape(-1, K))\n",
    "ypc_train_scaled = ypc_train_scaled.reshape(-1, M, K)  # Remove \", 1\"\n",
    "#ypc_train_scaled= np.where(ypc_train_scaled <= 0, 1e-30, ypc_train_scaled)\n",
    "#ypc_train_scaled = np.log(ypc_train_scaled)\n",
    "\n",
    "# For validation targets\n",
    "ypc_val_scaled = scaler_y_5.transform(ypc_val.reshape(-1, K))\n",
    "ypc_val_scaled = ypc_val_scaled.reshape(-1, M, K)  \n",
    "#ypc_val_scaled= np.where(ypc_val_scaled <= 0, 1e-30, ypc_val_scaled)\n",
    "#ypc_val_scaled = np.log(ypc_val_scaled)\n",
    "\n",
    "print('beta',X_train[0])\n",
    "print('betanorm',X_train_normalized[0])\n",
    "print('eitta',ypc_train[0])\n",
    "print('eitta_scaled',ypc_train_scaled[0])\n",
    "\n",
    "# ==============================================\n",
    "# Training Configuration\n",
    "# ==============================================\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define SWA callback\n",
    "# Corrected SWA callback\n",
    "swa_callback = ModelCheckpoint(\n",
    "    'swa_model.weights.h5',  # Changed extension\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "# Define OriginalScaleMAE callback\n",
    "class OriginalScaleMAE(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred = self.model.predict(X_val_normalized)\n",
    "        y_pred_original = scaler_y_5.inverse_transform(y_pred.reshape(-1, K))\n",
    "        y_true_original = scaler_y_5.inverse_transform(ypc_val_scaled.reshape(-1, K))\n",
    "        mae = np.mean(np.abs(y_pred_original - y_true_original))\n",
    "        print(f\"Original Scale MAE: {mae:.4f}\")\n",
    "\n",
    "# Callbacks list\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(monitor='val_mae', patience=5, restore_best_weights=True),\n",
    "    tf.keras.callbacks.ModelCheckpoint('PC_model_5.keras', save_best_only=True, monitor='val_mae'),  # Full model save\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_mae', factor=0.2, patience=5, verbose=1),\n",
    "    tf.keras.callbacks.TerminateOnNaN(),\n",
    "    ModelCheckpoint('swa_model.weights.h5', save_weights_only=True),  # Weights-only save\n",
    "    OriginalScaleMAE()\n",
    "]\n",
    "\n",
    "print('we will train the model now , be ready')\n",
    "print(PC_model_5.output_shape)\n",
    "joblib.dump(scaler_X_5, 'minmax_x_5.save')\n",
    "joblib.dump(scaler_y_5, 'minmax_y_5.save')\n",
    "# Assuming:\n",
    "# - X_train_normalized: Preprocessed training inputs (shape: (samples, 25, 1, 6))\n",
    "# - ypc_train_scaled: Scaled training targets (shape: (samples, 25, 6))\n",
    "# - X_val_normalized: Preprocessed validation inputs\n",
    "# - ypc_val_scaled: Scaled validation targets\n",
    "\n",
    "history = PC_model_5.fit(\n",
    "    X_train_normalized,  # Training inputs\n",
    "    ypc_train_scaled,    # Training targets\n",
    "    validation_data=(X_val_normalized, ypc_val_scaled),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "PC_model_5.save(f'PC_model_5.keras')\n",
    "print('trained ^ ^')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a9aeb-de44-4150-a544-0b84bf49c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting configuration\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# ======================\n",
    "# Power Control Metrics\n",
    "# ======================\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Power Control Loss (MSE)')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Training MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Power Control MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c396e5-616e-4196-acb3-1f4c47b07a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# 1. Load Model and Scalers\n",
    "#PC_model_5 = tf.keras.models.load_model('PC_model_5.keras')\n",
    "PC_model_5 = tf.keras.models.load_model(\n",
    "    'PC_model_5.keras',\n",
    "    custom_objects={'hybrid_loss': hybrid_loss}\n",
    ")\n",
    "\n",
    "scaler_X_5 = joblib.load('minmax_x_5.save')  # QuantileTransformer\n",
    "scaler_y_5 = joblib.load('minmax_y_5.save')  # PowerTransformer\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "def hybrid_loss(y_true, y_pred, delta=0.1, alpha=0.5):\n",
    "    huber = tf.keras.losses.Huber(delta=delta)(y_true, y_pred)\n",
    "    msle = tf.keras.losses.MeanSquaredLogarithmicError()(y_true, y_pred)\n",
    "    return alpha * huber + (1 - alpha) * msle\n",
    "\n",
    "\n",
    "def batch_predict(raw_betas):  # raw_betas shape: (N, 25, 6)\n",
    "    # Preprocess inputs\n",
    "    beta_normalized = scaler_X_5.transform(raw_betas.reshape(-1, K))\n",
    "    beta_normalized = beta_normalized.reshape(-1, M, 1, K)\n",
    "    # Predict\n",
    "    pc_norm = PC_model_5.predict(beta_normalized)\n",
    "    # Inverse transform targets\n",
    "    #print('pcnorm',pc_norm)\n",
    "    ypc_pred_original = scaler_y_5.inverse_transform(pc_norm.reshape(-1, K))\n",
    "    return ypc_pred_original.reshape(-1, M, K)  # Shape: (N, 25, 6)\n",
    "\n",
    "# Define M and K\n",
    "M, K = 25, 6\n",
    "\n",
    "# Load new beta data\n",
    "Path_Beta_predict = r\"D:\\OneDrive - Assuit University\\My_files\\Master Material\\Papers\\P5_Joint_Powercontrol_PilotAssignement\\Beta_test_148.csv\"\n",
    "Beta_mk_T_Predict = pd.read_csv(Path_Beta_predict, header=None)\n",
    "new_beta = Beta_mk_T_Predict.values.reshape(-1, M, K)\n",
    "\n",
    "#Beta_mk_T_Predict = X_train[0]\n",
    "# Convert to numpy arrays\n",
    "#new_beta = Beta_mk_T_Predict.reshape(-1, M, K)\n",
    "\n",
    "# Get predictions\n",
    "pc_output = batch_predict(new_beta)\n",
    "#print(\"Predicted Power Control Coefficients (Original Scale):\\n\", pc_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30410d2-d38c-4c23-80dd-bdddaed2f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Wrap the array in a dictionary with a MATLAB variable name (e.g., \"pc_coefficients\")\n",
    "data_dict = {'pc_output_5': pc_output}\n",
    "\n",
    "# Save the dictionary to .mat file\n",
    "scipy.io.savemat('pc_output_v5.mat', data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc090159-73d5-42f9-8d2a-84b030de3d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
